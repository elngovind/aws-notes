# CloudWatch Alarms and Notifications

## Chapter 51: Building Intelligent Alerting (December 2023)

### The Alert Fatigue Crisis
Two weeks into their CloudWatch implementation, MyLearning.com faced a new problem: their operations team was drowning in alerts. What started as a solution to improve visibility had become a source of constant interruption and stress.

*"We went from having no alerts to having too many alerts. At 3 AM, when you're getting your 47th notification about a minor CPU spike, you start to ignore everything. That's when we learned that good alerting is an art, not just a science."* - Priya (CPO)

```
Alert Fatigue Analysis - December 2023:
â”œâ”€â”€ Alert Volume Statistics
â”‚   â”œâ”€â”€ Week 1: 23 alerts (manageable)
â”‚   â”œâ”€â”€ Week 2: 156 alerts (concerning)
â”‚   â”œâ”€â”€ Week 3: 342 alerts (overwhelming)
â”‚   â”œâ”€â”€ Week 4: 89 alerts (after optimization)
â”‚   â””â”€â”€ Target: <50 meaningful alerts per week
â”œâ”€â”€ Alert Categories
â”‚   â”œâ”€â”€ ğŸš¨ Critical (Service Down): 5% of alerts
â”‚   â”œâ”€â”€ âš ï¸ Warning (Performance Degraded): 25% of alerts
â”‚   â”œâ”€â”€ ğŸ“Š Informational (Threshold Crossed): 70% of alerts
â”‚   â””â”€â”€ ğŸ”„ Recovery (Service Restored): Auto-generated
â”œâ”€â”€ Team Impact
â”‚   â”œâ”€â”€ Sleep Disruption: 15 after-hours alerts per week
â”‚   â”œâ”€â”€ Context Switching: 3.2 hours daily spent on false alarms
â”‚   â”œâ”€â”€ Alert Fatigue: 67% of alerts ignored after week 3
â”‚   â””â”€â”€ Burnout Risk: High stress levels in operations team
â””â”€â”€ Optimization Results
    â”œâ”€â”€ 74% reduction in alert volume
    â”œâ”€â”€ 95% improvement in alert relevance
    â”œâ”€â”€ 80% faster incident response time
    â””â”€â”€ Significant improvement in team morale
```

## Understanding CloudWatch Alarms

### Alarm Architecture and States
```
CloudWatch Alarms Fundamentals:

ğŸš¨ ALARM COMPONENTS
â”œâ”€â”€ Alarm Definition:
â”‚   â”œâ”€â”€ ğŸ“Š Metric: The data source being monitored
â”‚   â”œâ”€â”€ ğŸ¯ Threshold: The value that triggers the alarm
â”‚   â”œâ”€â”€ ğŸ“ Comparison Operator: >, <, >=, <=
â”‚   â”œâ”€â”€ â° Evaluation Period: How long to evaluate the condition
â”‚   â”œâ”€â”€ ğŸ“ˆ Statistic: Average, Sum, Maximum, Minimum, SampleCount
â”‚   â””â”€â”€ ğŸ”„ Datapoints to Alarm: How many periods must breach threshold
â”œâ”€â”€ Alarm States:
â”‚   â”œâ”€â”€ ğŸŸ¢ OK: Metric is within acceptable range
â”‚   â”œâ”€â”€ ğŸ”´ ALARM: Metric has breached threshold
â”‚   â”œâ”€â”€ ğŸŸ¡ INSUFFICIENT_DATA: Not enough data to evaluate
â”‚   â””â”€â”€ State transitions trigger actions and notifications
â”œâ”€â”€ Alarm Actions:
â”‚   â”œâ”€â”€ ğŸ“§ SNS Notifications: Email, SMS, HTTP endpoints
â”‚   â”œâ”€â”€ ğŸ¤– Auto Scaling Actions: Scale up/down EC2 instances
â”‚   â”œâ”€â”€ âš¡ EC2 Actions: Stop, terminate, reboot instances
â”‚   â”œâ”€â”€ ğŸ”„ Systems Manager Actions: Run automation documents
â”‚   â””â”€â”€ ğŸ“Š Custom Actions: Lambda functions, webhooks
â””â”€â”€ Alarm Types:
    â”œâ”€â”€ Metric Alarms: Based on single metric thresholds
    â”œâ”€â”€ Composite Alarms: Combine multiple alarm states
    â”œâ”€â”€ Anomaly Detection Alarms: ML-based threshold detection
    â””â”€â”€ Log Metric Filter Alarms: Based on log pattern matching
```

### Designing Effective Alerting Strategies

#### The Alert Hierarchy Framework
```
MyLearning.com Alert Severity Framework:

ğŸš¨ CRITICAL ALERTS (P0 - Immediate Response Required)
â”œâ”€â”€ Criteria:
â”‚   â”œâ”€â”€ Service completely unavailable
â”‚   â”œâ”€â”€ Data loss or corruption risk
â”‚   â”œâ”€â”€ Security breach indicators
â”‚   â”œâ”€â”€ Revenue-impacting failures
â”‚   â””â”€â”€ Compliance violations
â”œâ”€â”€ Examples:
â”‚   â”œâ”€â”€ Website completely down (HTTP 5xx > 50%)
â”‚   â”œâ”€â”€ Database connection failures (100% failure rate)
â”‚   â”œâ”€â”€ Payment processing failures (>10% failure rate)
â”‚   â”œâ”€â”€ Authentication system down
â”‚   â””â”€â”€ Data backup failures
â”œâ”€â”€ Response Requirements:
â”‚   â”œâ”€â”€ â° Response Time: <5 minutes
â”‚   â”œâ”€â”€ ğŸ“ Notification: Phone call + SMS + Email
â”‚   â”œâ”€â”€ ğŸ‘¥ Escalation: Immediate manager notification
â”‚   â”œâ”€â”€ ğŸ“‹ Actions: Automated recovery + manual intervention
â”‚   â””â”€â”€ ğŸ“Š Documentation: Incident report required
â””â”€â”€ Notification Channels:
    â”œâ”€â”€ Primary: Phone calls to on-call engineer
    â”œâ”€â”€ Secondary: SMS to entire operations team
    â”œâ”€â”€ Tertiary: Email to management and stakeholders
    â””â”€â”€ Backup: Slack channel with @here mention

âš ï¸ WARNING ALERTS (P1 - Urgent Response Required)
â”œâ”€â”€ Criteria:
â”‚   â”œâ”€â”€ Service degraded but functional
â”‚   â”œâ”€â”€ Performance below SLA thresholds
â”‚   â”œâ”€â”€ Resource utilization approaching limits
â”‚   â”œâ”€â”€ Error rates elevated but manageable
â”‚   â””â”€â”€ Capacity planning concerns
â”œâ”€â”€ Examples:
â”‚   â”œâ”€â”€ API response time > 2 seconds (p95)
â”‚   â”œâ”€â”€ CPU utilization > 85% for 10 minutes
â”‚   â”œâ”€â”€ Database connections > 90% of limit
â”‚   â”œâ”€â”€ Disk space > 90% utilization
â”‚   â””â”€â”€ Error rate > 1% for 5 minutes
â”œâ”€â”€ Response Requirements:
â”‚   â”œâ”€â”€ â° Response Time: <15 minutes
â”‚   â”œâ”€â”€ ğŸ“ Notification: SMS + Email
â”‚   â”œâ”€â”€ ğŸ‘¥ Escalation: After 30 minutes if unresolved
â”‚   â”œâ”€â”€ ğŸ“‹ Actions: Investigation and mitigation
â”‚   â””â”€â”€ ğŸ“Š Documentation: Issue tracking required
â””â”€â”€ Notification Channels:
    â”œâ”€â”€ Primary: SMS to on-call engineer
    â”œâ”€â”€ Secondary: Email to operations team
    â”œâ”€â”€ Tertiary: Slack channel notification
    â””â”€â”€ Escalation: Manager notification after 30 minutes

ğŸ“Š INFORMATIONAL ALERTS (P2 - Monitoring and Trending)
â”œâ”€â”€ Criteria:
â”‚   â”œâ”€â”€ Metrics approaching warning thresholds
â”‚   â”œâ”€â”€ Unusual but not critical patterns
â”‚   â”œâ”€â”€ Capacity planning indicators
â”‚   â”œâ”€â”€ Performance optimization opportunities
â”‚   â””â”€â”€ Business metric anomalies
â”œâ”€â”€ Examples:
â”‚   â”œâ”€â”€ CPU utilization > 70% for 30 minutes
â”‚   â”œâ”€â”€ Memory usage trending upward
â”‚   â”œâ”€â”€ Unusual traffic patterns detected
â”‚   â”œâ”€â”€ Cache hit ratio declining
â”‚   â””â”€â”€ Student enrollment rate changes
â”œâ”€â”€ Response Requirements:
â”‚   â”œâ”€â”€ â° Response Time: <2 hours (business hours)
â”‚   â”œâ”€â”€ ğŸ“ Notification: Email only
â”‚   â”œâ”€â”€ ğŸ‘¥ Escalation: None required
â”‚   â”œâ”€â”€ ğŸ“‹ Actions: Analysis and planning
â”‚   â””â”€â”€ ğŸ“Š Documentation: Optional tracking
â””â”€â”€ Notification Channels:
    â”œâ”€â”€ Primary: Email to operations team
    â”œâ”€â”€ Secondary: Dashboard updates
    â”œâ”€â”€ Tertiary: Weekly summary reports
    â””â”€â”€ Archive: Historical trend analysis
```

### Alarm Configuration Best Practices

#### Threshold Setting and Evaluation Periods
```
Intelligent Alarm Configuration:

ğŸ¯ THRESHOLD SETTING STRATEGIES
â”œâ”€â”€ Statistical Approach:
â”‚   â”œâ”€â”€ Baseline Establishment: 30-day historical analysis
â”‚   â”œâ”€â”€ Standard Deviation: Mean + 2Ïƒ for warning, Mean + 3Ïƒ for critical
â”‚   â”œâ”€â”€ Percentile-Based: 95th percentile for performance metrics
â”‚   â”œâ”€â”€ Business Impact: Thresholds based on SLA requirements
â”‚   â””â”€â”€ Seasonal Adjustment: Account for traffic patterns and cycles
â”œâ”€â”€ MyLearning.com Threshold Examples:
â”‚   â”œâ”€â”€ CPU Utilization:
â”‚   â”‚   â”œâ”€â”€ Warning: >75% for 10 minutes (allows for brief spikes)
â”‚   â”‚   â”œâ”€â”€ Critical: >90% for 5 minutes (immediate attention needed)
â”‚   â”‚   â””â”€â”€ Rationale: Based on auto-scaling trigger points
â”‚   â”œâ”€â”€ API Response Time:
â”‚   â”‚   â”œâ”€â”€ Warning: >1 second p95 for 5 minutes
â”‚   â”‚   â”œâ”€â”€ Critical: >3 seconds p95 for 2 minutes
â”‚   â”‚   â””â”€â”€ Rationale: User experience impact thresholds
â”‚   â”œâ”€â”€ Error Rate:
â”‚   â”‚   â”œâ”€â”€ Warning: >0.5% for 5 minutes
â”‚   â”‚   â”œâ”€â”€ Critical: >2% for 2 minutes
â”‚   â”‚   â””â”€â”€ Rationale: Acceptable failure rate vs user impact
â”‚   â””â”€â”€ Database Connections:
â”‚       â”œâ”€â”€ Warning: >80% of max connections for 10 minutes
â”‚       â”œâ”€â”€ Critical: >95% of max connections for 2 minutes
â”‚       â””â”€â”€ Rationale: Connection pool exhaustion prevention
â””â”€â”€ Dynamic Threshold Adjustment:
    â”œâ”€â”€ Time-based: Different thresholds for peak vs off-peak hours
    â”œâ”€â”€ Load-based: Adjust thresholds based on current traffic volume
    â”œâ”€â”€ Seasonal: Account for exam periods and enrollment cycles
    â””â”€â”€ Machine Learning: Use CloudWatch Anomaly Detection

â° EVALUATION PERIOD OPTIMIZATION
â”œâ”€â”€ Period Selection Guidelines:
â”‚   â”œâ”€â”€ High-frequency metrics (CPU, Memory): 1-5 minute periods
â”‚   â”œâ”€â”€ Medium-frequency metrics (API calls): 5-15 minute periods
â”‚   â”œâ”€â”€ Low-frequency metrics (Daily users): 1-24 hour periods
â”‚   â”œâ”€â”€ Business metrics (Revenue): 1-24 hour periods
â”‚   â””â”€â”€ Batch job metrics: Job duration-based periods
â”œâ”€â”€ Datapoints to Alarm Strategy:
â”‚   â”œâ”€â”€ Transient Issues: Require 2-3 consecutive breaches
â”‚   â”œâ”€â”€ Persistent Issues: Require 1-2 breaches out of 3-5 periods
â”‚   â”œâ”€â”€ Critical Systems: Lower threshold (1 out of 2 periods)
â”‚   â”œâ”€â”€ Noisy Metrics: Higher threshold (3 out of 5 periods)
â”‚   â””â”€â”€ Business Metrics: Longer evaluation (5 out of 10 periods)
â””â”€â”€ Missing Data Treatment:
    â”œâ”€â”€ Treat Missing Data as Breaching: For critical availability metrics
    â”œâ”€â”€ Treat Missing Data as Not Breaching: For optional performance metrics
    â”œâ”€â”€ Treat Missing Data as Ignore: For intermittent batch processes
    â””â”€â”€ Context-dependent: Based on metric importance and frequency
```

### SNS Integration and Notification Channels

#### Multi-Channel Notification Strategy
```
Comprehensive Notification Architecture:

ğŸ“§ SNS TOPIC ORGANIZATION
â”œâ”€â”€ Topic Structure:
â”‚   â”œâ”€â”€ ğŸš¨ mylearning-critical-alerts
â”‚   â”‚   â”œâ”€â”€ Subscribers: On-call engineers (SMS + Email)
â”‚   â”‚   â”œâ”€â”€ Escalation: Management team (Email)
â”‚   â”‚   â”œâ”€â”€ Integration: PagerDuty, Slack, Phone calls
â”‚   â”‚   â””â”€â”€ Delivery Policy: Immediate, retry on failure
â”‚   â”œâ”€â”€ âš ï¸ mylearning-warning-alerts
â”‚   â”‚   â”œâ”€â”€ Subscribers: Operations team (Email + Slack)
â”‚   â”‚   â”œâ”€â”€ Filtering: Business hours vs after-hours
â”‚   â”‚   â”œâ”€â”€ Integration: Ticketing system, Slack channels
â”‚   â”‚   â””â”€â”€ Delivery Policy: Standard, no retry needed
â”‚   â”œâ”€â”€ ğŸ“Š mylearning-info-alerts
â”‚   â”‚   â”œâ”€â”€ Subscribers: Development team (Email only)
â”‚   â”‚   â”œâ”€â”€ Batching: Hourly digest during business hours
â”‚   â”‚   â”œâ”€â”€ Integration: Dashboard updates, reports
â”‚   â”‚   â””â”€â”€ Delivery Policy: Best effort, no urgency
â”‚   â””â”€â”€ ğŸ”„ mylearning-recovery-alerts
â”‚       â”œâ”€â”€ Subscribers: All stakeholders (Email)
â”‚       â”œâ”€â”€ Purpose: Incident resolution confirmation
â”‚       â”œâ”€â”€ Integration: Status page updates
â”‚       â””â”€â”€ Delivery Policy: Confirmation of service restoration
â”œâ”€â”€ Subscription Management:
â”‚   â”œâ”€â”€ Role-based subscriptions (Engineer, Manager, Executive)
â”‚   â”œâ”€â”€ Time-based filtering (Business hours, After-hours, Weekends)
â”‚   â”œâ”€â”€ Severity-based routing (Critical â†’ Phone, Warning â†’ Email)
â”‚   â””â”€â”€ Geographic distribution (Follow-the-sun support model)
â””â”€â”€ Message Formatting:
    â”œâ”€â”€ Subject Line: [SEVERITY] Service - Brief Description
    â”œâ”€â”€ Body Content: Timestamp, Metric, Threshold, Current Value, Actions
    â”œâ”€â”€ Runbook Links: Direct links to troubleshooting procedures
    â””â”€â”€ Dashboard Links: Quick access to relevant monitoring dashboards
```

#### Advanced Notification Features
```
Enhanced Notification Capabilities:

ğŸ”” SMART NOTIFICATION FEATURES
â”œâ”€â”€ Message Filtering and Routing:
â”‚   â”œâ”€â”€ Attribute-based filtering (Environment, Service, Severity)
â”‚   â”œâ”€â”€ Time-based routing (Business hours vs after-hours)
â”‚   â”œâ”€â”€ Geographic routing (Regional on-call teams)
â”‚   â””â”€â”€ Escalation policies (Auto-escalate after time threshold)
â”œâ”€â”€ Delivery Optimization:
â”‚   â”œâ”€â”€ Delivery Status Tracking: Confirm message receipt
â”‚   â”œâ”€â”€ Retry Policies: Automatic retry on delivery failure
â”‚   â”œâ”€â”€ Dead Letter Queues: Handle undeliverable messages
â”‚   â””â”€â”€ Delivery Delay: Batch non-critical notifications
â”œâ”€â”€ Integration Capabilities:
â”‚   â”œâ”€â”€ ğŸ“± Mobile Push Notifications: AWS Mobile SDK integration
â”‚   â”œâ”€â”€ ğŸ’¬ Slack Integration: Direct channel posting with formatting
â”‚   â”œâ”€â”€ ğŸ“ Voice Calls: Amazon Connect integration for critical alerts
â”‚   â”œâ”€â”€ ğŸ“Š Ticketing Systems: JIRA, ServiceNow automatic ticket creation
â”‚   â””â”€â”€ ğŸ”— Webhook Integration: Custom endpoints for third-party tools
â””â”€â”€ Message Enhancement:
    â”œâ”€â”€ Rich Formatting: HTML email templates with charts and graphs
    â”œâ”€â”€ Contextual Information: Related metrics and historical trends
    â”œâ”€â”€ Action Buttons: Direct links to remediation actions
    â””â”€â”€ Acknowledgment Tracking: Confirm alert receipt and response
```

### Composite Alarms and Dependencies

#### Building Intelligent Alarm Hierarchies
```
Composite Alarm Architecture:

ğŸ”— ALARM DEPENDENCY MODELING
â”œâ”€â”€ Service Health Composite Alarms:
â”‚   â”œâ”€â”€ Web Application Health:
â”‚   â”‚   â”œâ”€â”€ Component Alarms: ALB health + EC2 health + Database health
â”‚   â”‚   â”œâ”€â”€ Logic: AND condition (all components must be healthy)
â”‚   â”‚   â”œâ”€â”€ Benefit: Single alert for overall service status
â”‚   â”‚   â””â”€â”€ Use Case: Executive dashboard and status page updates
â”‚   â”œâ”€â”€ Database Cluster Health:
â”‚   â”‚   â”œâ”€â”€ Component Alarms: Primary DB + Read Replica + Connection Pool
â”‚   â”‚   â”œâ”€â”€ Logic: OR condition (any component failure triggers alert)
â”‚   â”‚   â”œâ”€â”€ Benefit: Comprehensive database monitoring
â”‚   â”‚   â””â”€â”€ Use Case: Database administrator notifications
â”‚   â””â”€â”€ Payment System Health:
â”‚       â”œâ”€â”€ Component Alarms: API Gateway + Lambda + External Gateway
â”‚       â”œâ”€â”€ Logic: Complex condition with weighted importance
â”‚       â”œâ”€â”€ Benefit: Business-critical system monitoring
â”‚       â””â”€â”€ Use Case: Revenue protection and compliance
â”œâ”€â”€ Geographic Redundancy Alarms:
â”‚   â”œâ”€â”€ Multi-Region Service Health:
â”‚   â”‚   â”œâ”€â”€ Component Alarms: Primary region + DR region status
â”‚   â”‚   â”œâ”€â”€ Logic: Both regions down = critical, one region down = warning
â”‚   â”‚   â”œâ”€â”€ Benefit: Disaster recovery status monitoring
â”‚   â”‚   â””â”€â”€ Use Case: Business continuity assurance
â”‚   â””â”€â”€ Load Balancer Failover:
â”‚       â”œâ”€â”€ Component Alarms: Primary LB + Secondary LB health
â”‚       â”œâ”€â”€ Logic: Automatic failover trigger conditions
â”‚       â”œâ”€â”€ Benefit: Seamless traffic routing
â”‚       â””â”€â”€ Use Case: High availability maintenance
â””â”€â”€ Business Process Alarms:
    â”œâ”€â”€ Student Enrollment Process:
    â”‚   â”œâ”€â”€ Component Alarms: Registration + Payment + Course Access
    â”‚   â”œâ”€â”€ Logic: Sequential dependency chain
    â”‚   â”œâ”€â”€ Benefit: End-to-end process monitoring
    â”‚   â””â”€â”€ Use Case: Revenue funnel optimization
    â””â”€â”€ Exam Delivery Process:
        â”œâ”€â”€ Component Alarms: Authentication + Content Delivery + Submission
        â”œâ”€â”€ Logic: Critical path analysis
        â”œâ”€â”€ Benefit: Student experience protection
        â””â”€â”€ Use Case: Academic integrity assurance
```

### Reducing Alert Fatigue

#### Alert Optimization Strategies
```
Alert Fatigue Reduction Techniques:

ğŸ¯ ALERT QUALITY IMPROVEMENT
â”œâ”€â”€ Alert Consolidation:
â”‚   â”œâ”€â”€ Time-based Grouping: Batch similar alerts within time windows
â”‚   â”œâ”€â”€ Service-based Grouping: Combine related service alerts
â”‚   â”œâ”€â”€ Severity-based Grouping: Escalate only after multiple warnings
â”‚   â””â”€â”€ Geographic Grouping: Regional alert consolidation
â”œâ”€â”€ Intelligent Suppression:
â”‚   â”œâ”€â”€ Maintenance Windows: Suppress alerts during planned maintenance
â”‚   â”œâ”€â”€ Dependency-based: Suppress downstream alerts when upstream fails
â”‚   â”œâ”€â”€ Correlation-based: Suppress duplicate alerts from related metrics
â”‚   â””â”€â”€ Time-based: Suppress non-critical alerts during off-hours
â”œâ”€â”€ Dynamic Thresholding:
â”‚   â”œâ”€â”€ Machine Learning: CloudWatch Anomaly Detection integration
â”‚   â”œâ”€â”€ Seasonal Adjustment: Account for predictable traffic patterns
â”‚   â”œâ”€â”€ Load-based Scaling: Adjust thresholds based on current load
â”‚   â””â”€â”€ Historical Analysis: Continuous threshold optimization
â””â”€â”€ Alert Lifecycle Management:
    â”œâ”€â”€ Auto-resolution: Automatically resolve alerts when conditions clear
    â”œâ”€â”€ Escalation Policies: Progressive notification intensity
    â”œâ”€â”€ Acknowledgment Tracking: Confirm alert receipt and action
    â””â”€â”€ Feedback Loop: Learn from false positives and adjust

ğŸ“Š ALERT EFFECTIVENESS METRICS
â”œâ”€â”€ Alert Quality KPIs:
â”‚   â”œâ”€â”€ Alert-to-Incident Ratio: Target <5 alerts per real incident
â”‚   â”œâ”€â”€ False Positive Rate: Target <10% of all alerts
â”‚   â”œâ”€â”€ Mean Time to Acknowledge: Target <5 minutes for critical alerts
â”‚   â”œâ”€â”€ Alert Resolution Time: Target <15 minutes for warnings
â”‚   â””â”€â”€ Alert Fatigue Score: Survey-based team satisfaction metric
â”œâ”€â”€ Response Quality Metrics:
â”‚   â”œâ”€â”€ First Response Time: How quickly team responds to alerts
â”‚   â”œâ”€â”€ Resolution Accuracy: Percentage of correctly diagnosed issues
â”‚   â”œâ”€â”€ Escalation Rate: How often alerts require management involvement
â”‚   â””â”€â”€ Repeat Alert Rate: How often same issues trigger multiple alerts
â””â”€â”€ Business Impact Correlation:
    â”œâ”€â”€ Revenue-affecting Alerts: Alerts that correlate with revenue loss
    â”œâ”€â”€ Customer-affecting Alerts: Alerts that impact user experience
    â”œâ”€â”€ SLA-affecting Alerts: Alerts that threaten service level agreements
    â””â”€â”€ Compliance-affecting Alerts: Alerts related to regulatory requirements
```

### MyLearning.com Alarm Implementation

#### Production Alarm Configuration
```
MyLearning.com Critical Alarm Setup:

ğŸš¨ TIER 1: BUSINESS-CRITICAL ALARMS
â”œâ”€â”€ Service Availability:
â”‚   â”œâ”€â”€ Website Down: HTTP 5xx errors >50% for 2 minutes
â”‚   â”œâ”€â”€ API Unavailable: API Gateway 5xx errors >25% for 1 minute
â”‚   â”œâ”€â”€ Database Down: RDS connection failures >90% for 1 minute
â”‚   â””â”€â”€ Payment Failures: Payment API errors >10% for 2 minutes
â”œâ”€â”€ Performance Degradation:
â”‚   â”œâ”€â”€ Slow Response: API p95 latency >3 seconds for 5 minutes
â”‚   â”œâ”€â”€ High Error Rate: Application errors >2% for 3 minutes
â”‚   â”œâ”€â”€ Database Slow: Query latency >1 second for 5 minutes
â”‚   â””â”€â”€ Cache Miss: Redis hit ratio <70% for 10 minutes
â””â”€â”€ Security Incidents:
    â”œâ”€â”€ Failed Logins: >100 failed attempts per minute
    â”œâ”€â”€ Unusual Access: Geographic anomaly detection
    â”œâ”€â”€ Data Breach: Unauthorized data access patterns
    â””â”€â”€ DDoS Attack: Request rate >10x normal for 2 minutes

âš ï¸ TIER 2: OPERATIONAL ALARMS
â”œâ”€â”€ Resource Utilization:
â”‚   â”œâ”€â”€ High CPU: >85% utilization for 10 minutes
â”‚   â”œâ”€â”€ High Memory: >90% utilization for 10 minutes
â”‚   â”œâ”€â”€ Disk Space: >90% utilization for 5 minutes
â”‚   â””â”€â”€ Network Saturation: >80% bandwidth for 15 minutes
â”œâ”€â”€ Capacity Planning:
â”‚   â”œâ”€â”€ Connection Pool: >80% database connections for 15 minutes
â”‚   â”œâ”€â”€ Queue Depth: >1000 background jobs for 10 minutes
â”‚   â”œâ”€â”€ Storage Growth: >10GB daily growth for 3 days
â”‚   â””â”€â”€ Traffic Growth: >50% increase sustained for 1 hour
â””â”€â”€ Application Health:
    â”œâ”€â”€ Slow Queries: Database queries >500ms for 10 minutes
    â”œâ”€â”€ Memory Leaks: Gradual memory increase over 4 hours
    â”œâ”€â”€ Thread Pool: >90% thread utilization for 5 minutes
    â””â”€â”€ Garbage Collection: GC time >10% for 15 minutes

ğŸ“Š TIER 3: INFORMATIONAL ALARMS
â”œâ”€â”€ Business Metrics:
â”‚   â”œâ”€â”€ Low Enrollment: <50% of daily target by 6 PM
â”‚   â”œâ”€â”€ High Churn: >5% daily user churn rate
â”‚   â”œâ”€â”€ Poor Engagement: <60% course completion rate
â”‚   â””â”€â”€ Revenue Decline: <90% of daily revenue target
â”œâ”€â”€ Performance Trends:
â”‚   â”œâ”€â”€ Response Time Trend: 20% increase over 7-day average
â”‚   â”œâ”€â”€ Error Rate Trend: 50% increase over 24-hour average
â”‚   â”œâ”€â”€ Traffic Pattern: Unusual traffic distribution
â”‚   â””â”€â”€ Resource Trend: 30% increase in resource usage over 3 days
â””â”€â”€ Operational Metrics:
    â”œâ”€â”€ Deployment Frequency: <2 deployments per week
    â”œâ”€â”€ Test Coverage: <80% code coverage
    â”œâ”€â”€ Technical Debt: >20% of development time on maintenance
    â””â”€â”€ Support Tickets: >50% increase in ticket volume
```

---

*"Good alerting is like a good conversationâ€”it tells you what you need to know, when you need to know it, without overwhelming you with noise. It took us time to learn that less can definitely be more."* - Raj (CTO)

### Key Takeaways

1. **Alert Hierarchy**: Implement clear severity levels with appropriate response requirements
2. **Threshold Intelligence**: Use statistical analysis and business impact to set meaningful thresholds
3. **Notification Strategy**: Multi-channel notifications with role-based routing and escalation
4. **Composite Alarms**: Model service dependencies to reduce alert noise and improve context
5. **Continuous Optimization**: Regularly review and tune alerts to maintain effectiveness and reduce fatigue